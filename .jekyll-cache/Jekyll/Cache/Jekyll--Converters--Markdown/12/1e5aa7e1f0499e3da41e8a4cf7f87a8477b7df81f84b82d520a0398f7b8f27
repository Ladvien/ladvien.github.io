I"˜ <p>I‚Äôm writing learning-notes from implementing a ‚Äútoxic comment‚Äù detector using a convolutional neural network (CNN).  This is a common project across the interwebs, however, the articles I‚Äôve seen on the matter leave a few bits out.  So, I‚Äôm attempting to augment public knowledge‚Äìnot write a comprehensive tutorial.</p>

<p>A common omission is what the data look like as they travel through pre-processing.  I‚Äôll try to show how the data look before falling into the neural-net black-hole.  However, I‚Äôll stop short before reviewing the CNN setup, as this is explained much better elsewhere.  Though, I‚Äôve put all the original code, relevant project links, tutorial links, and other resources towards the bottom.</p>

<h2 id="the-code">The Code</h2>

<h4 id="code-imports">Code: Imports</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">GlobalMaxPooling1D</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">MaxPooling1D</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.initializers</span> <span class="kn">import</span> <span class="n">Constant</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="n">api</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>

</code></pre></div></div>
<p>The above code includes several packages which would need to be downloaded.  The easiest way is to use <a href="https://www.w3schools.com/python/python_pip.asp">pip</a>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install keras
pip install gensim
pip install pandas
</code></pre></div></div>

<h4 id="code-variables">Code: Variables</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BASE_DIR</span> <span class="o">=</span> <span class="s">'your project directory'</span>
<span class="n">TRAIN_TEXT_DATA_DIR</span> <span class="o">=</span> <span class="n">BASE_DIR</span> <span class="o">+</span> <span class="s">'train.csv'</span>
<span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">MAX_NUM_WORDS</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">VALIDATION_SPLIT</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div></div>
<p>The above variables define the preprocessing actions and the neural-network.</p>

<h4 id="train_text_data_dir">TRAIN_TEXT_DATA_DIR</h4>
<p>The directory containing the data file <code class="highlighter-rouge">train.csv</code></p>

<h4 id="max_sequence_length">MAX_SEQUENCE_LENGTH</h4>
<p>The toxic_comment data set contains comments collected from Wikipedia.  MAX_SEQUENCE_LENGTH is used in the preprocessing stages to truncate a comment if too long.  That is, greater than <code class="highlighter-rouge">MAX_SEQUENCE_LENGTH</code>.  For example, a comment like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You neeed to @#$ you mother!$@#$&amp;...
</code></pre></div></div>
<p>Probably doesn‚Äôt need much more for the network to discern it‚Äôs a toxic comment.  Also, if we create the network based around the longest comment, it will become unnecessarily large and slow. Much like the human brain (See <a href="https://en.wikipedia.org/wiki/Overchoice#cite_note-4">Overchoice</a>), we need to provide as little information as needed to make a good decision.</p>

<h4 id="max_num_words">MAX_NUM_WORDS</h4>
<p>This variable is the maximum number of words to include‚Äìor, vocabulary size.</p>

<p>Much like truncating the sequence length, the maximum vocabulary should not be overly inclusive.  The number <code class="highlighter-rouge">20,000</code> comes from a ‚Äústudy‚Äù stating an average person only uses 20,000 words.  Of course, I‚Äôve not found a primary source stating this‚Äìnot saying it‚Äôs not out there, but I‚Äôve not found it yet. (Halfhearted search results in the appendix.)</p>

<p>Regardless, it seems to help us justify keeping the NN nimble.</p>

<h4 id="embedding_dim">EMBEDDING_DIM</h4>
<p>In my code, I‚Äôve used <a href="https://radimrehurek.com/gensim/">gensim</a> to download pre-trained word embeddings.  But  beware, not all pre-trained embeddings have the same number of dimensions.  This variables defines the size of the embeddings used.  <strong>Please note, if you use embeddings other than <code class="highlighter-rouge">glove-wiki-gigaword-300</code> you will need to change this variable to match.</strong></p>

<h4 id="validation_split">VALIDATION_SPLIT</h4>
<p>A helper function in Keras will split our data into a <code class="highlighter-rouge">test</code> and <code class="highlighter-rouge">validation</code>.  This percentage represents how much of the data to hold back for validation.</p>

<h4 id="code-load-embeddings">Code: Load Embeddings</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Loading word vectors.'</span><span class="p">)</span>
<span class="c1"># Load embeddings
</span><span class="n">info</span> <span class="o">=</span> <span class="n">api</span><span class="p">.</span><span class="n">info</span><span class="p">()</span>
<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">api</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"glove-wiki-gigaword-300"</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="highlighter-rouge">info</code> object is a list of <a href="https://radimrehurek.com/gensim/">gensim</a> embeddings available.  You can use any of the listed embeddings in the format <code class="highlighter-rouge">api.load('name-of-desired-embedding')</code>.  One nice feature of <a href="https://radimrehurek.com/gensim/">gensim</a>‚Äôs <code class="highlighter-rouge">api.load</code> is it will automatically download the embeddings from the Internet and load them into Python.  Of course, once they‚Äôve been downloaded, <a href="https://radimrehurek.com/gensim/">gensim</a> will load the local copy.  This makes it easy to experiment with different embedding layers.</p>

<h4 id="code-process-embeddings">Code: Process Embeddings</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">index2word</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="p">.</span><span class="n">index2word</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">embedding_model</span><span class="p">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">word2idx</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
    <span class="n">word2idx</span><span class="p">[</span><span class="n">index2word</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span> <span class="o">=</span> <span class="n">index</span>
</code></pre></div></div>

<p>The two dictionaries <code class="highlighter-rouge">index2word</code> and <code class="highlighter-rouge">word2idx</code> are key to embeddings.</p>

<p>The <code class="highlighter-rouge">word2idx</code> is a dictionary where the keys are the words contained in the embedding and the values are the integers they represent.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"the"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">","</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">"."</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">"of"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s">"to"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s">"and"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="p">....</span>
    <span class="s">"blah"</span><span class="p">:</span> <span class="mi">12984</span><span class="p">,</span>
    <span class="p">...</span>
<span class="p">}</span>  
</code></pre></div></div>
<p><code class="highlighter-rouge">index2word</code> is a list where the the values are the words and the word‚Äôs position in the string represents it‚Äôs index in the <code class="highlighter-rouge">word2idx</code>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">index2word</span> <span class="o">=</span> <span class="p">[</span><span class="s">"the"</span><span class="p">,</span> <span class="s">","</span><span class="p">,</span> <span class="s">"."</span><span class="p">,</span> <span class="s">"of"</span><span class="p">,</span> <span class="s">"to"</span><span class="p">,</span> <span class="s">"and"</span><span class="p">,</span> <span class="p">...]</span>
</code></pre></div></div>
<p>These will be used to turn our comment strings into integer vectors.</p>

<p>After this bit of code we should have three objects.</p>

<ol>
  <li><code class="highlighter-rouge">embedding_model</code> ‚Äì Pre-trained relationships between words, which is a matrix 300 x 400,000.</li>
  <li><code class="highlighter-rouge">index2word</code> ‚Äì A dictionary containing <code class="highlighter-rouge">key-value</code> pairs, the key being the word as a string and value being the integer representing the word.  Note, these integers correspond with the index in the <code class="highlighter-rouge">embedding_model</code>.</li>
  <li><code class="highlighter-rouge">word2idx</code> ‚Äì A list containing all the words.  The index corresponds to the word‚Äôs position in the word embeddings.  Essentially, the reverse of the <code class="highlighter-rouge">index2word</code>.
<img src="../images/embeddings_1.png" alt="" /></li>
</ol>

<h4 id="code-get-toxic-comments-labels">Code: Get Toxic Comments Labels</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Loading Toxic Comments data.'</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">TRAIN_TEXT_DATA_DIR</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">toxic_comments</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">TRAIN_TEXT_DATA_DIR</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Getting Comment Labels.'</span><span class="p">)</span>
<span class="n">prediction_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">"toxic"</span><span class="p">,</span> <span class="s">"severe_toxic"</span><span class="p">,</span> <span class="s">"obscene"</span><span class="p">,</span> <span class="s">"threat"</span><span class="p">,</span> <span class="s">"insult"</span><span class="p">,</span> <span class="s">"identity_hate"</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">toxic_comments</span><span class="p">[</span><span class="n">prediction_labels</span><span class="p">].</span><span class="n">values</span>

</code></pre></div></div>
<p>This loads the <code class="highlighter-rouge">toxic_comment.csv</code> as a Pandas dataframe called <code class="highlighter-rouge">toxic_comments</code>.  We then grab all of the comment labels using their column names.  This becomes a second a numpy matrix called <code class="highlighter-rouge">labels</code>.</p>

<p>We will use the text in the <code class="highlighter-rouge">toxic_comments</code> dataframe to predict the data found in the <code class="highlighter-rouge">labels</code> matrix.  That is, <code class="highlighter-rouge">toxic_comments</code> will be our <code class="highlighter-rouge">x_train</code> and <code class="highlighter-rouge">labels</code> our <code class="highlighter-rouge">y_train</code>.</p>

<p>You may notice, the labels are also included in our <code class="highlighter-rouge">toxic_comments</code>.  But they will not be used, as we will only be taking the <code class="highlighter-rouge">comment_text</code> column to become our <code class="highlighter-rouge">sequences</code> here in a moment.</p>

<h4 id="toxic_comments-dataframe"><code class="highlighter-rouge">toxic_comments</code> dataframe</h4>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>id</th>
      <th>comment_text</th>
      <th>toxic</th>
      <th>severe_toxic</th>
      <th>obscene</th>
      <th>threat</th>
      <th>insult</th>
      <th>identity_hate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5</td>
      <td>00025465d4725e87</td>
      <td>Congratulations from me as well, use the tools well.  ¬∑ talk</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0002bcb3da6cb337</td>
      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>7</td>
      <td>00031b1e95af7921</td>
      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don‚Äôt do it again, or you will be banned.</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<h4 id="labels-y_train-numpy-matrix"><code class="highlighter-rouge">labels</code> (<code class="highlighter-rouge">y_train</code>) numpy matrix</h4>

<table>
  <thead>
    <tr>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<h4 id="code-convert-comments-to-sequences">Code: Convert Comments to Sequences</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Tokenizing and sequencing text.'</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">MAX_NUM_WORDS</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">toxic_comments</span><span class="p">[</span><span class="s">'comment_text'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">"&lt;DT&gt;"</span><span class="p">).</span><span class="n">values</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">toxic_comments</span><span class="p">[</span><span class="s">'comment_text'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">"&lt;DT&gt;"</span><span class="p">).</span><span class="n">values</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Found %s sequences.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
</code></pre></div></div>
<p>The <code class="highlighter-rouge">Tokenizer</code> object comes from the <code class="highlighter-rouge">Keras</code> API.  It takes chunks of texts cleans them and then converts them to unique integer values.</p>

<ul>
  <li><a href="https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/">keras.preprocessing.text.Tokenizer</a></li>
</ul>

<p>The <code class="highlighter-rouge">num_words</code> argument tells the Tokenizer to only preserve the word frequencies higher than this threshold.  This makes it necessary to run the <code class="highlighter-rouge">fit()</code> on the targeted texts before using the Tokenizer.  The fit function will determine the number of occurrences each word has throughout all the texts provided, then, it will order these by frequency.  This frequency rank can be found in the <code class="highlighter-rouge">tokenizer.word_index</code> property.</p>

<p>For example, looking at the dictionary below, if <code class="highlighter-rouge">num_words</code> = 7 all words after ‚Äúi‚Äù would be excluded.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"the"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s">"to"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">"of"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s">"and"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s">"a"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s">"you"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="s">"i"</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="s">"is"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="p">...</span>
    <span class="s">"hanumakonda"</span><span class="p">:</span> <span class="mi">210334</span><span class="p">,</span>
    <span class="s">"956ce"</span><span class="p">:</span> <span class="mi">210335</span><span class="p">,</span>
    <span class="s">"automakers"</span><span class="p">:</span> <span class="mi">210336</span><span class="p">,</span>
    <span class="s">"ciu"</span><span class="p">:</span> <span class="mi">210337</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Also, as we are loading the data, we are filling any missing values with a dummy token (i.e., ‚Äú&lt;DT&gt;‚Äù).  This probably isn‚Äôt the <em>best</em> way to handle missing values, however, given the amount of data, it‚Äôs probably best to try and train the network using this method.  Then, come back and handle <code class="highlighter-rouge">na</code> values more strategically.  Diminishing returns and all that.</p>

<h4 id="code-padding">Code: Padding</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>
</code></pre></div></div>
<p>This is an easy one.  It pads our sequences so they are all the same length.  The <a href="https://keras.io/preprocessing/sequence/">pad_sequences</a> function is part of the Keras library.  A couple of important arguments have default values: <code class="highlighter-rouge">padding</code> and <code class="highlighter-rouge">truncating</code>.</p>

<p>Here‚Äôs the Keras docs explanation:</p>

<blockquote>
  <p>padding: String, ‚Äòpre‚Äô or ‚Äòpost‚Äô: pad either before or after each sequence.</p>
</blockquote>

<blockquote>
  <p>truncating: String, ‚Äòpre‚Äô or ‚Äòpost‚Äô: remove values from sequences larger than  maxlen, either at the beginning or at the end of the sequences.</p>
</blockquote>

<p>Both arguments default to <code class="highlighter-rouge">pre</code>.</p>

<p>Lastly, the <code class="highlighter-rouge">maxlen</code> argument controls where padding and truncation happen.  And we are setting it with our <code class="highlighter-rouge">MAX_SEQUENCE_LENGTH</code> variable.</p>

<p><img src="../images/embeddings_3.png" alt="padding-sequences-before-after" /></p>

<h4 id="code-applying-embeddings">Code: Applying Embeddings</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_words</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">MAX_NUM_WORDS</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_words</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="p">.</span><span class="n">get_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">continue</span>
</code></pre></div></div>

<p>Here‚Äôs where stuff gets good.  The code above will take all the words from our <code class="highlighter-rouge">tokenizer</code>, look up the word-embedding (vector) for each word, then add this to the <code class="highlighter-rouge">embedding matrix</code>.  The <code class="highlighter-rouge">embedding_matrix</code> will be converted into a <code class="highlighter-rouge">keras.layer.Embeddings</code> object.</p>

<ul>
  <li><a href="https://keras.io/layers/embeddings/">Embeddings</a></li>
</ul>

<p>I think of an <code class="highlighter-rouge">Embedding</code> layer as a transformation tool sitting at the top of our neural-network.  It takes the integer representing a word and outputs its word-embedding vector.  It then passes the vector into the neural-network.  Simples!</p>

<p>Probably best to visually walk through what‚Äôs going on.  But first, let‚Äôs talk about the code before the <code class="highlighter-rouge">for-loop</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_words</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">MAX_NUM_WORDS</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div>
<p>This gets the maximum number of words to be addeded in our embedding layer.  If it is less than our ‚Äúaverage English speaker‚Äôs vocabulary‚Äù‚Äì20,000‚Äìwe‚Äôll use all of the words found in our tokenizer.  Otherwise, the <code class="highlighter-rouge">for-loop</code> will stop after <code class="highlighter-rouge">num_words</code> is met.  And remember, the <code class="highlighter-rouge">tokenizer</code> has kept the words in order of their frequency‚Äìso, the words which are lost aren‚Äôt as critical.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_words</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">))</span>
</code></pre></div></div>
<p>This initializes our embedding_matrix, which is a <code class="highlighter-rouge">numpy</code> object with all values set to zero.  Note, if the <code class="highlighter-rouge">EMBEDDING_DIM</code> size does not match the size of the word-embeddings loaded, the code will execute, but you will get a bad embedding matrix.  Further, you might not notice until your network isn‚Äôt training.  I mean, not that this happened to <em>me</em>‚ÄìI‚Äôm just guessing it could happen to <em>someone</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="p">.</span><span class="n">get_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">continue</span>
</code></pre></div></div>

<p>Here‚Äôs where the magic happens.  The <code class="highlighter-rouge">for-loop</code> iterates over the words in the <code class="highlighter-rouge">tokenizer</code> object <code class="highlighter-rouge">word_index</code>.  It attempts to find the word in word-embeddings, and if it does, it adds the vector to the embedding matrix at a row respective to its index in the <code class="highlighter-rouge">word_index</code> object.</p>

<p>Confused? Me too.  Let‚Äôs visualize it.</p>

<p>Let‚Äôs walk through the code with a word in mind: ‚Äúof‚Äù.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
</code></pre></div></div>
<p>By now the <code class="highlighter-rouge">for-loop</code> is two words in.  The words ‚Äúthe‚Äù and ‚Äúto‚Äù have already been added.  Therefore, for this iteration <code class="highlighter-rouge">word</code> = ‚Äòof‚Äô and <code class="highlighter-rouge">i</code> = 2.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="p">.</span><span class="n">get_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</code></pre></div></div>
<p>The the word-embedding for the word ‚Äúof‚Äù is</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-0.076947, -0.021211, 0.21271, -0.72232, -0.13988, -0.12234, ...
</code></pre></div></div>
<p>This list is contained in a <code class="highlighter-rouge">numpy.array</code> object.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</code></pre></div></div>

<p>Lastly, the word-embedding vector representing  ‚Äúof‚Äù gets added to the third row of the embedding matrix (the matrix index starts at 0).</p>

<p>Here‚Äôs how the embedding matrix should look after the word ‚Äúof‚Äù is added. (The first column added for readability.)</p>

<table>
  <thead>
    <tr>
      <th>word</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>‚Ä¶</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>the</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>‚Ä¶</td>
    </tr>
    <tr>
      <td>to</td>
      <td>0.04656</td>
      <td>0.21318</td>
      <td>-0.0074364</td>
      <td>-0.45854</td>
      <td>‚Ä¶</td>
    </tr>
    <tr>
      <td>of</td>
      <td>-0.25756</td>
      <td>-0.057132</td>
      <td>-0.6719</td>
      <td>-0.38082</td>
      <td>‚Ä¶</td>
    </tr>
    <tr>
      <td>‚Ä¶</td>
      <td>‚Ä¶</td>
      <td>‚Ä¶</td>
      <td>‚Ä¶</td>
      <td>‚Ä¶</td>
      <td>‚Ä¶</td>
    </tr>
  </tbody>
</table>

<p>Also, for a deep visualization, check the image above.  The picture labeled ‚Äúword embeddings‚Äù is <em>actually</em> the output of our <code class="highlighter-rouge">embedding_matrix</code>.  The big difference? The word vectors in the <code class="highlighter-rouge">gensim</code> embedding_model which are not found anywhere in our corpus (all the text contained in the toxic_comments column) have been replaced with all zeroes.</p>

<p><img src="../images/embeddings_2.png" alt="embedding-matrix" /></p>

<h4 id="code-creating-embedding-layer">Code: Creating Embedding Layer</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word2idx</span><span class="p">),</span>
                            <span class="n">EMBEDDING_DIM</span><span class="p">,</span>
                            <span class="n">embeddings_initializer</span><span class="o">=</span><span class="n">Constant</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">),</span>
                            <span class="n">input_length</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,</span>
                            <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>
<p>Here we are creating the first layer of our NN.  The primary parameter passed into the Keras <code class="highlighter-rouge">Embedding</code> class is the <code class="highlighter-rouge">embedding_matrix</code>, which we created above.  However, there are several other attributes of the <code class="highlighter-rouge">embedding_layer</code> we must define. Keep in mind our <code class="highlighter-rouge">embedding_layer</code> will take an integer representing a word as input and output a vector, which is the word-embedding.</p>

<p>First, the <code class="highlighter-rouge">embedding_layers</code> needs to know the input dimensions.  The input dimension is the number of words we are considering for this training session.  This can be found by taking the length of our <code class="highlighter-rouge">word2idx</code> object.  So, the <code class="highlighter-rouge">len(word2idx)</code> returns the total number of words to consider.</p>

<p>One note on the layer‚Äôs input, there are two ‚Äúinput‚Äù arguments for <code class="highlighter-rouge">keras.layers.Embedding</code> class initializer, which can be confusing.  They are <code class="highlighter-rouge">input</code> and <code class="highlighter-rouge">input_length</code>. The <code class="highlighter-rouge">input</code> is the number of possible values provided to the layer.  The <code class="highlighter-rouge">input_length</code> is how many values will be passed in a sequence.</p>

<p>Here are the descriptions from the Keras documentation:</p>

<p><code class="highlighter-rouge">input</code></p>
<blockquote>
  <p>int &gt; 0. Size of the vocabulary, i.e. maximum integer index + 1.</p>
</blockquote>

<p><code class="highlighter-rouge">input_length</code></p>
<blockquote>
  <p>Length of input sequences, when it is constant. This argument is required if you are going to connect  Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).</p>
</blockquote>

<p>In our case, the <code class="highlighter-rouge">input</code> will be the vocabulary size and <code class="highlighter-rouge">input_length</code> is the number of words in a sequence, which should be <code class="highlighter-rouge">MAX_SEQUENCE_LENGTH</code>.  This is also why we padded comments shorter than <code class="highlighter-rouge">MAX_SEQUENCE_LENGTH</code>, as the embedding layer will expect a consistent size.</p>

<p>Next, the <code class="highlighter-rouge">embedding_layers</code> needs to know the dimensions of the output.  The output is going to be a word-embedding vector, which <em>should</em> be the same size as the word embeddings loaded from the <a href="https://radimrehurek.com/gensim/">gensim</a> library.<br />
We defined this size with the <code class="highlighter-rouge">EMBEDDING_DIM</code> variable.</p>

<p>Lastly, the <code class="highlighter-rouge">training</code> option is set to <code class="highlighter-rouge">False</code> so the word-embedding relationships are not updated as we train our <code class="highlighter-rouge">toxic_comment</code> detector.  You could set it to <code class="highlighter-rouge">True</code>, but come on, let‚Äôs be honest, are we going to be doing better than Google?</p>

<h4 id="code-splitting-the-data">Code: Splitting the Data</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nb_validation_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">VALIDATION_SPLIT</span> <span class="o">*</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="n">nb_validation_samples</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="o">-</span><span class="n">nb_validation_samples</span><span class="p">]</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="n">nb_validation_samples</span><span class="p">:]</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">-</span><span class="n">nb_validation_samples</span><span class="p">:]</span>
</code></pre></div></div>
<p>Here we are forming our data as inputs.  We convert the <code class="highlighter-rouge">data</code> into <code class="highlighter-rouge">x_train</code> and <code class="highlighter-rouge">x_val</code>.  The <code class="highlighter-rouge">labels</code> dataframe becomes <code class="highlighter-rouge">y_train</code> and <code class="highlighter-rouge">y_val</code>.  And here marks the end of <strong>pre-processing.</strong></p>

<p>But! Let‚Äôs recap before you click away:</p>

<ol>
  <li>Load the word-embeddings.  These are pre-trained word relationships.  It is a matrix 300 x 400,000.</li>
  <li>Create two look up objects: <code class="highlighter-rouge">index2word</code> and <code class="highlighter-rouge">word2idx</code></li>
  <li>Get our <code class="highlighter-rouge">toxic_comment</code> and <code class="highlighter-rouge">labels</code> data.</li>
  <li>Convert the <code class="highlighter-rouge">comments</code> column from <code class="highlighter-rouge">toxic_comments</code> dataframe into the <code class="highlighter-rouge">sequences</code> list.</li>
  <li>Create a <code class="highlighter-rouge">tokenizer</code> object and fit it to the <code class="highlighter-rouge">sequences</code> text</li>
  <li>Pad all the sequences so they are the same size.</li>
  <li>Look up the word-embedding vector for each unique word in <code class="highlighter-rouge">sequences</code>.  Store the word-embedding vector in th<code class="highlighter-rouge">embedding_matrix</code>.  If the word is not found in the embeddings, then leave the index all zeroes.  Also, limit the embedding-matrix to the 20,000 most used words.</li>
  <li>Create a Keras <code class="highlighter-rouge">Embedding</code> layer from the <code class="highlighter-rouge">embedding_matrix</code></li>
  <li>Split the data for training and validation.</li>
</ol>

<p>And that‚Äôs it.  The the prepared <code class="highlighter-rouge">embedding_layer</code> will become the first layer in the network.</p>

<h4 id="code-training">Code: Training</h4>
<p>Like I stated at the beginning, I‚Äôm not going to review training the network, as there are many better explanations‚Äìand I‚Äôll link them in the Appendix.  However, for those interested, here‚Äôs the rest of the code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GlobalMaxPooling1D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prediction_labels</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Training model.'</span><span class="p">)</span>
<span class="c1"># happy learning!
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</code></pre></div></div>

<p>Oh! There‚Äôs one more bit I‚Äôd like to go over, which most other articles have left out.  Prediction.</p>

<h4 id="code-predictions">Code: Predictions</h4>
<p>I mean, training a CNN is fun and all, but how does one use it?  Essentially, it comes down to repeating the steps above, but with with less data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">prediction_labels</span><span class="p">):</span>
    <span class="c1"># Convert the sequence to tokens and pad it.
</span>    <span class="n">sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
    <span class="n">sequence</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>

    <span class="c1"># Make a prediction
</span>    <span class="n">sequence_prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Take only the first of the batch of predictions
</span>    <span class="n">sequence_prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sequence_prediction</span><span class="p">).</span><span class="nb">round</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Label the predictions
</span>    <span class="n">sequence_prediction</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">prediction_labels</span>
    <span class="k">return</span> <span class="n">sequence_prediction</span>

<span class="c1"># Create a test sequence
</span><span class="n">sequence</span> <span class="o">=</span> <span class="p">[</span><span class="s">"""
            Put your test sentence here.
            """</span><span class="p">]</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">create_prediction</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">,</span> <span class="n">prediction_labels</span><span class="p">)</span>
</code></pre></div></div>

<p>The function above needs the following arguments:</p>
<ul>
  <li>The pre-trained <code class="highlighter-rouge">model</code>.  This is the Keras model we just trained.</li>
  <li>A <code class="highlighter-rouge">sequence</code> you‚Äôd like to determine whether it is ‚Äútoxic‚Äù.</li>
  <li>The <code class="highlighter-rouge">tokenizer</code>, which is used to encode the prediction sequence the same way as the training sequences.</li>
  <li><code class="highlighter-rouge">max_length</code> must be the same as the maximum size of the training sequences</li>
  <li>The <code class="highlighter-rouge">prediction_labels</code> are a list of strings containing the human readable labels for the predicted tags (e.g. ‚Äútoxic‚Äù, ‚Äúsevere_toxic‚Äù, ‚Äúinsult‚Äù, etc.)</li>
</ul>

<p>Really, the function takes all the important parts of our pre-processing and reuses them on the prediction sequence.</p>

<p>One piece of the function you might tweak is the <code class="highlighter-rouge">.round(0)</code>.  I‚Äôve put this there to convert the predictions into binary.  That is, if prediction for a sequence is <code class="highlighter-rouge">.78</code> it is rounded up to <code class="highlighter-rouge">1</code>.  This is do to the binary nature of the prediction.  Either a comment is toxic or it is not.  Either <code class="highlighter-rouge">0</code> or <code class="highlighter-rouge">1</code>.</p>

<p>Well, that‚Äôs what I got.  Thanks for sticking it out.  Let me know if you have any questions.</p>

<h3 id="appendix">Appendix</h3>

<h4 id="full-code">Full Code</h4>

<ul>
  <li><a href="https://github.com/Ladvien/nn_learning_cnn_toxic_comment">toxic_comment.py</a></li>
</ul>

<h4 id="tutorials">Tutorials</h4>
<ul>
  <li><a href="https://arxiv.org/pdf/1802.09957.pdf">Convolutional Neural Networks for Toxic Comment Classification (Academic)</a></li>
  <li><a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels?sortBy=relevance&amp;group=everyone&amp;search=cnn&amp;page=1&amp;pageSize=20&amp;competitionId=8076">Kaggle Projects Using a CNN and Toxicty Data</a></li>
  <li><a href="https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/">Tutorial of Using Word Vectors</a></li>
  <li><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">Keras Tutorial on Using Pretrained Word Embeddings</a></li>
</ul>

<p>If you want to know more about <a href="https://radimrehurek.com/gensim/">gensim</a> and how it can be used with Keras.</p>
<ul>
  <li><a href="https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/">Depends on the Definition</a></li>
</ul>

<h4 id="data">Data</h4>
<p>The data are hosted by Kaggle.</p>

<ul>
  <li><a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data">Wikipedia‚Äôs ‚ÄúToxic Comment‚Äù Data</a></li>
</ul>

<p>Please note, you will have to sign-up for a Kaggle account.</p>

<h4 id="average-persons-vocabulary-size">Average Person‚Äôs Vocabulary Size</h4>
<p>Primary sources on vocabulary size:</p>
<ul>
  <li><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01116/full">How Many Words Do We Know? Practical Estimates of Vocabulary Size Dependent on Word Definition, the Degree of Language Input and the Participant‚Äôs Age</a></li>
  <li><a href="https://www.victoria.ac.nz/lals/about/staff/publications/paul-nation/1990-Goulden-Voc-size.pdf">How Large Can a Receptive Vocabulary Be?</a></li>
  <li><a href="https://journals.sagepub.com/doi/abs/10.1080/10862969109547729">Toward a Meaningful Definition of Vocabulary Size</a></li>
  <li><a href="http://centaur.reading.ac.uk/29879/">Vocabulary size revisited: the link between vocabulary size and academic achievement</a></li>
  <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4965448/">How Many Words Do We Know? Practical Estimates of Vocabulary Size Dependent on Word Definition, the Degree of Language Input and the Participant‚Äôs Age
</a></li>
</ul>
:ET