---
layout: article
title: HM-1X Aid
categories: robots
excerpt:
tags: [robots]
color: "#152a55"
image:
  feature: HM-10-2.jpg
  teaser: HM-10-2.jpg
  thumb:
comments: true
---

###Overview

Well, I've had the urge to hack.  It's been strong for awhile now, sadly, with a more than fullt-time job and Bek in graduate school, I've just had no time.  **Until now!**  The new job I have (HMIS Database Manager) has _actual_ vacation time.  I've had almost two weeks off (combined with the Christmas holiday).  The first few days were obviously spent working...**But!** After turning my phone and email I was able to...catch up on family time.  Sigh.  Then, clean house.  Then, get bored.  But with a few days left I actually got some hacking in.

I downloaded Visual Studio Express on my work computer a few months ago.  (Shh.  Don't tell the boss.) But I've not had time to write a single line of code.  This holiday was a good time to learn C\#!

Two weeks later I had this monster,

<div class="flex-video">
<iframe width="420" height="315" src="https://www.youtube.com/embed/XtrS7ENc1EU" frameborder="0" allowfullscreen></iframe>
</div>

It is meant to be a GUI for the HM-10, HM-11, and HM-15 modules.  The highlights,

[HM-1X Aid Download](https://github.com/Ladvien/HM-1X-Build/raw/master/setup.exe)

The source can be found,

[HM-1X Aid Source](https://github.com/Ladvien/HM-1X_Aid_v01)

1. It uses threads to prevent any "Sleeping."  Keeps the UI healthy.
2. IO.SerialPorts _should_ allow a lot of USB-to-UART chips to work.
3. Basic Terminal App features (view as ASCII, HEC, Decimal, etc.) like RealTerm and the others. BUT! With one feature I've longed for in a terminal app for so damn long.  The ability to save your settings on exit.  No more selecting 9600 baud for the billionth time.
4. I've put a lot of command validity checks in the system.  For example, if you were to type "AT+CON0123**S**6789012" Would not be a valid command normally, but terminal will convert the "S" to "F."
5. I have also imbued it with a bit my understanding of the HM-1X datasheets.  This is probably the greatest selling point of the program, sadly.
6. **C# is my new favorite.  Don't judge me :P.**

I thought I would take some time to go into the code involved in the little terminal, not for posterity, merely for my own reference.

_Quick reminder, for anyone unfamiliar with my posts:_
*I am not a profesionnal.  These writings are not great.  They are simply my journal towards understanding what the hell I'm doing*

### Object Oriented Programming

I have not posted as much lately.  It is a combination of losing LMR and not having time since Bek started school.  But I have definitely been writing code.  The focus has been on learning OOP design.  This this C\# program was probably my first _real_ object-oriented program.  But I have also been writing in [Swift](https://developer.apple.com/ios/?cid=wwa-us-kwg-features) for iOS, which is an OOP language as well.

Arlight, so what's the difference between OOP and the other thing? And why learn OOP?  I thought robots used microcontollers which are much too small to handle OOP?  Well, I'm learning every robot builder is _already_ an object oriented programmer.

#### Difference between OOP and Procedural programming

I wont butcher others' explanations by rephrasing them. Instead, I'm going to list what helped me understand.

1. [Object-Oriented programming (Wikipedia)](https://en.wikipedia.org/wiki/Object-oriented_programming)
2. [Object-Oriented programming](https://www.youtube.com/watch?v=lbXsrHGhBAU) (video)
3. [Procedural Programming](https://www.youtube.com/watch?v=b4n9KYD05jg) (video)
4. [Derek Banas' Design Patterns Series](https://www.youtube.com/watch?v=vNHpsC5ng_E&list=PLF206E906175C7E07) (examples in Java, but it helped me the most.  Got to see OOP in action.)

I believe every roboticist is both an object oriented programmer and a procedural programmer.  They create objects with specific functions (PCBs, STLs, etc.), holding on to the plans so copies can be made.  They instantiate those objects (print the STL, send the PCB file to OSHPark, etc).  Each of these objects created usually has a specific purpose.  The design of the object, a motor-controller for instance, is often only accessible by a specific input such as UART connection. Its outputs are controlled by how the inputs are digested by the motor-controller itself. The robot-builder may decide to add an accelerometer to the motor-controller by copying the design files and adding the needed accelerometer circuit (inheritance).  And so forth.

It seems like a lot of the the concepts are supported by this metaphor,

1. [Abstraction](https://en.wikipedia.org/wiki/Abstraction_%28computer_science%29)
2. [Encapsulation](https://en.wikipedia.org/wiki/Encapsulation_%28computer_programming%29)
3. [Polymorhphism](https://en.wikipedia.org/wiki/Polymorphic_code)
4. [Inheritance](https://en.wikipedia.org/wiki/Inheritance_%28object-oriented_programming%29)

Objects are great.  Especially when they are walking, talking, grasping robotic sort of objects.  However, each roboticist must line the insides of objects with procedures that move data through in a predictable manner.  This means, at some granularity, objects are filled with small runs of procedural programming.  Ultimately, it takes both for roboticists, object oriented programming and procedural programming.  And I argue, whether aware or not, the roboticist practices both continuously.

Moving away from the hippy-dippy stuff; for my personal development as a robot builder I will be taking time to learn both, regardless of my possibly metaphor of convenience, as they both appear as roots of computational-thinking

In application, I want to know procedural programming to be effective. Microcontrollers often have small memory sets and the code needs to move from input to output with little overhead (hmm, procedural programming is a linear style and a line is the shortest distance between two points, there's gotta be something there).  But I want to know how to connect my robot to large systems, such as a Raspberry Pi, PC, or the whooooole Internet.  To do this effectively I need to be able to pass data between procedural and object based programs.

### Avoiding Sleep

<img height="110" width="160" style="float: left; margin: 10px;" src="http://www.frugal-cafe.com/public_html/frugal-blog/frugal-cafe-blogzone/wp-content/uploads/2010/11/coffee-smiley-face-ballistik-coffee-boy-flickr.jpg">

My program is walking between the microcontroller world and the big-boy-PC world.  As I stated above, at some point the HM-1X module would need to pass its data to the PC. Here in lies a dilemma best explained by a picture,

<img height="277" width="640" style="margin: 10px; alignment: center;" src="/images/threadingCars.png">

For the HM-1X Aid the data is passed through the Serial connection.  Of course, as I stated above, I am using the Systems.IO.Ports.SerialPorts framework to handle the incoming serial data.  A nifty little aspect of IO.Ports is it actually has the [DataReceivedEvent](https://msdn.microsoft.com/en-us/library/system.io.ports.serialport.datareceived%28v=vs.110%29.aspx) on a separate thread (more on threads in a moment).  This event is triggered by an RX interrupt on whatever USB-to-[UART](https://en.wikipedia.org/wiki/Universal_asynchronous_receiver/transmitter) chip, which allows data to be handled as it is coming in.

Now, I mentioned methods for handling data probably go from procedural to object-oriented when moving upstream from a microcontroller to a PC.  A USB-to-UART bridge is a perfect example.  Receiving [UART](https://en.wikipedia.org/wiki/Universal_asynchronous_receiver/transmitter) data for a microntroller looks like this,

{% highlight c %}
int incomingByte = 0;   // for incoming serial data

void setup() {
        Serial.begin(9600);     // opens serial port, sets data rate to 9600 bps
}

void loop() {
        // send data only when you receive data:
        if (Serial.available() > 0) {
                // read the incoming byte:
                incomingByte = Serial.read();
                // say what you got:
                Serial.print("I received: ");
                Serial.println(incomingByte, DEC);
        }
}
{% endhighlight %}

Here, the microcontroller is looping over a if-statement to see if there is any data available in the RX buffer.  Whenever the `loop()` finds there are data available, then it will run through some procedures on handling those data.  This method works OK, your microcontroller's program is only dealing with serial data.  But what happens if is supposed to handle other tasks and your microcontroller has a never-ending stream of data?  Whatever code is after the `if(Serial.available() > 0)` will not execute, since the available serial data will never be less than 0.  This is referred to as using a "blocking" serial data method.  Not sure how this term was derived, but I'm guessing "blocking" comes from how the method prevents the program from doing anything else

Hmm, wouldn't be better to handle serial data "immediately" when it comes in?  That's where interrupts come in.  

The `Serial.onReceive()` is an interrupt vector which fires every time the serial data is received.  The interrupt vector calls a method which copies the data received from the serial buffer into the `string_buffer`.

{% highlight c %}
void MyFunction(){
   int i;
   int length = Serial.available();
   int string_buffer[32];   

   //copy data out of the receive buffer
   for(i = 0; i < length; i++){
      string_buffer[i] = Serial.read();
   }

   //run a string compare function, or otherwise parse the received data
   if(MySpecialStingCompare(string_buffer,"Hello Arduino")){
      Serial.println("Hello World");
   }
}

void setup(){
   Serial.begin(9600);
   Serial.onReceive(MyFunction);
}

void loop(){
   //do nothing
}

{% endhighlight %}

This is a non-blocking method of handling serial data.  It takes advantage of a hardware level peripheral on the Atmel chips known as an [interrupt vector](https://en.wikipedia.org/wiki/Interrupt_vector_table).  This particular interrupt is fired any time the RX pin receives a series of LOWs which resemble incoming data.  

This method has the advantage of freeing the microcontroller to do other things in the main loop and handle data _only_ when new data is received.  This saves program from having to make a comparison every clock cycle.  Also, and more importantly, it allows the microcontroller to immediately update data important to the purpose of the main process; this is critical in processes which are time sensitive, such as remote control on a quadcopter.

For example, if we wrote a radio controller for our quadcopter using a Bluetooth device which talks to an Arduino Pro Mini.  If we used the blocking method the Arduino receives any data from the radio controller, like, oh I don't know, "Don't run into that tree!" the main process on the Arduino would not have this information until it gets back to the `Serial.available() > 0`.  Not cool if milliseconds matter.

However, if you were to write the same radio controller using non-blocking, interrupt based serial communication, then whenever you send the signal to the Bluetooth device, and that device sends it serially to the Arduino, the Arduino will basically bookmark its place in the main process and handle any data regarding its immediate crash.

Another example, what happens if your Arduino has an LCD which is meant to display the output of a temperature sensor.  Yet, the temperature sensor and display are a small portion of what the Arduino is doing.  If the temperature changes using the blocking methods, then the LCD will not be updated until the Arduino finishes whatever tasks and makes it back around to the `if(Serial.available() > 0)`. This will make your LCD's responsiveness be clunky.

Contrast this with the non-blocking method (see what I did there?).  Setting up an interrupt on the temperature sensor to update the LCD whenever the temperature changes will make LCD appear responsive to temperature changes.  This is how a 3D Printer handles the thousands of tasks it must complete and still keep its LCD responsive.

Good stuff.

Alright, so that's how a microcontroller handles things--even with interrupts it's still processing tasks one at a time.  We can avoid the appearance of slowing the microcontroller with processing intensive tasks, like waiting on serial data to be received, but ultimately, the microcontroller is handling the tasks one at a time.  So what does this have to do with my C# program?

Let's take a look at C\#'s version of 'Serial.onReceive()'

{% highlight c# %}
string InputData = "";

// Read Data.
private void DataReceivedHandler(object sender, SerialDataReceivedEventArgs e)
{
    // Gets incoming data from open port and passes it to a handler.
    InputData = ComPort.ReadExisting();
    captureBuffer = InputData;
    captureBuffer = "";

}
{% endhighlight %}

This is probably the simplest version of a `DataReceivedHandler()` using C\#.  In short, it fires whenever data is received; so it's non-blocking.  The method, as I have it written, simply grabs the data from the interrupt buffer using `ComPort.ReadExisting()` and shoves it into the `inputData`.  Pretty simple, right?  Being candid, I believe it is one reason so many [poo-poo](http://www.sparxeng.com/blog/software/must-use-net-system-io-ports-serialport) using IO.Ports.SerialPort framework.  But how is IO.Ports.SerialPorts different than the [Arduino HAL](http://hackaday.com/2015/10/20/the-case-for-arduino-in-real-engineering/) `Serial.onReceive()`?   Hmm, perhaps I should have used a more credible comparison.

But coming back to the complex command.  What if a computer sends a command to the microcontroller and expects a response?  It's going to take some time for the microcontroller to reply.  In the meantime, what does the PC do?  It could run off to do other tasks and wait for the onReceive to fire.  But, what if one of those tasks requires some information from microcontroller.  In short, the microcontroller is holding up the show and little can be done but wait.

We know the PC is going to need to wait.  Coming back to the blocking and non-blocking methods we looked at on microntroller, the only option really available in C\# is the non-blocking, which is good because it's our favorite.  But is it really enough?

If our PC program is serial-data centric, meaning the PC's processes are dependent on the microcontrollers data and vice-versa, then simply using the non-blocking helps us little.  The PC could move from waiting on serial data to painting a label on the UI, but what if the text in that label is meant to be data from the microcontroller?  Regardless of how one looks at it, it seems like it would be best for the sake of our program to throttle the PC to the microcontroller.

But exactly how much slower would our program go?  Let's look.

Let's say you have the following:

Arduino Uno (Atmega328P): 16mhz
PC: 2.2ghz
UART-to-USB: 9600bps

Let's find the common denominator and compare.

Arduino:

{% highlight xml %}
16mhz = 16 * 1,000,000 = 16,000,000
1 to 2 clock cycles for each instruction
1 to 2 / 16,000,000 = 62.5 to 125 nano-seconds  
(Who said Arduinos were slow? Pfft)
{% endhighlight %}


PC:

{% highlight xml %}
2.2ghz = 2.2 * 11,000,000,000 = 2,200,000,000
1 to 2 clock cycles for each instruction
1 to 2 / 2,200,000,000 = .45456 to .90900 nano-seconds
(Oh, I guess it's slow to the PC.)
{% endhighlight %}

UART-to-USB:

{% highlight xml %}
bytes x bits_per_character / bits_per_second
1 byte x 10 / 9600
10 / 9600 = 10416666.66 nano-seconds (~10 milliseconds)
{% endhighlight %}

Hmm, I guess we identified the hold up, eh?

This means even if we were limited to the Arduino speed we would probably be ok updating UIs on the PC without it appearing clunky.  But, limited to the UART-to-USB; well, crap.

The main difference is the the C\# data receive method is actually on a separate thread from the main thread.  One bit I noticed writing in object oriented programming is they often utilize [threading](https://en.wikipedia.org/wiki/Thread_%28computing%29).  Most microcontrollers have one core (yes, yes, except the Parrallax), so threading is not something done often on a microcontroller, but PCs are often take advantage of threads.

<img height="178" width="248" style="margin: 10px; float: right;" src="/images/quad_core_die_arduino.png">
Now, I'm going to dare to oversimplify what little understanding I have regarding threads.  If you are coming from the Arduino world, the best way to think of threading is having two Arduinos connected by I2C.  One of the Arduinos is the master and the other the slave.  Whenever the master Arduino gets data from the USB-to-UART there is code on the master which sends half the data to the slave Arduino via I2C, with some instructions it should do some particular tasks with the data and send the results back to the master Arduino.  While the slave works on its half of the data, the master is working on the other half.  If the slave gets done first (there is nothing dictating which order they finish in; it is asynchronous) the master has an interrupt on the I2C, it sees the slave is done, and tells him to wait to send its results.  After the master finishes, it sends a request for the completed data from the slave.  Lastly, if the master finishes first, then it waits with a dirty-look only 8-bit ICs can give while the slave finishes its crunching.  Ultimately, when all the data is crunched the main processor does something with it.  Each Arduino here would be a processor and the tasks running on them would be a thread.


<img height="178" width="248" style="margin: 10px; float: left;" src="/images/quad_core_die.jpeg">
Inside the PC we have something similar going on, but instead of two Arduninos you would have two processors on the same  die.

I like to think of a thread as a separate process within our system.  Of course, we can't call the second process, "main process."  We've already got one of those. And computers don't like ambiguity.  Instead, let's get creative and call the second process something wild, fun, and bizarre! Like..."second process."

Let me to try and explain how this is different than the Arduino HAL.  In C\# the second process is pretty much like the main process.  Don't be fooled, there _is_ a lot of complexity to it, but the compiler takes care of most of it.  With one grand honking exception: The user interface (UI) runs on the main process and cannot be updated by any other process.  This makes perfect sense; you wouldn't want half a button to show on your UI, right?  Instead, a protocol saving data in a space both processes can access it and a flag to look in the shared space when updated.  This is like the good old snail-mail mailbox where the flag is raised when you want the post-person to look for letter.

Bringing it back to my code.  The HM-1X's serial DataReceivedEvent is on one process and my UI is on another.  This means, when my program gets some data serially then it puts that data in the shared space and lets the UI know there data has been received.  

Those of you who are sharper than me (all of you?) will notice an issue _similar_ to the Arduino and its LCD.  If the data coming in and the UI are working out of sync what happens if a user does something like continually hit the Send on a command without waiting for a serial response to be received.  The best case scenario is we got an "OK" for every time the command was sent.  However, if it is a more complicated command which requires a back-and-forth between the microcontroller and the desktop, well, big problems.  It would be like a [https://www.youtube.com/watch?v=yCOPJi0Urq4()](sloth and a rabbit trying to have a conversation).  Those of you from the Arduino world know the common (however, I'll argue not best) answer to this dilemma: `delay()`.

The `delay()` in the Arduino HAL basically tells the microcontroller to do nothing for however long you tell it.  This allows the message to go across UART-to-USB and back and _then_ look for the answer.  But this is poor design.  I'm not focusing on how to do this in the Arduino HAL, but I'll say it involves interrupts and good code.  But let's take a look at what happens when you are trying to update Arduino's version of a UI the LED display.


1. Can't update UI when main process is busy.  Arduino and not updating an OLED until finished with Serial data.
2. Advantages of interrupts; received data immediately added to main process
3. Advantage of threads; off-load clock cycle intensive operations to a different process to maintain responsiveness of UI and other main process fun.
I dunno, I'm DATKing this article.


One last note, I started porting this to Mac and Linux using MonoDevelopment, but it would require writing my own interrupt handler.  Or faking it with a timeout method.  Nether option is worth it to me.  Too many other projects and too little time.
